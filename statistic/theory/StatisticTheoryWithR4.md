---
title: 实用统计学（四）
categories: [Statistic,Theory]
tags:
  - Statistic
  - R
toc: true
abbrlink: 181ef7d8
date: 2022-07-12 17:30:20
---
<!--more-->
## 回归 & 预测
回归可用于预测和解释

__回归主要用于传统的解释性建模而不是预测。__



概念|定义
---|---
响应变量（特征向量）|想要预测的变量。同义词：因变量、变量 Y、目标、结果
自变量（预测变量）|用于预测响应的变量。同义词：自变量、变量 X、特征、属性
记录|一个表示特定个体或实例的向量，由因子和结果值组成。同义词：行、案例、实例、示例
截距|回归线的截距，即当 X = 0 时的预测值。
回归系数|回归线的斜率。同义词：斜率、参数估计值、权重
拟合值|从回归线获得的估计值 。同义词：预测值
残差|观测值和拟合值之间的差异。同义词：误差
残差平方和|残差值的平方和（RSS）　
最小二乘法|一种通过最小化残差的平方和而拟合回归的方法。同义词：普通最小二乘法（OLS）回归
R使用lm函数拟合线性回归

#### 多元线性回归
概念|定义
---|---
均方根误差|回归均方误差的平方根，是比较回归模型时使用最广泛的度量。同义词：RMSE
标准残差|与均方根误差的计算一样，只是根据自由度做了调整。同义词：RSE
R 方|可以被模型解释的变异的比例，值介于 0 到 1 之间。同义词：决定系数、$R^2$
t 统计量|预测因子的系数，除以系数的标准误差。它提供了一种比较模型中变量重要性的度量。
加权回归|在回归中，记录具有不同的权重。
``` R
house_lm <- lm(AdjSalePrice ~ SqFtTotLiving + SqFtLot + Bathrooms +
              Bedrooms + BldgGrade,
              data=house, na.action=na.omit)
```

#### 模型评估
均方根误差（RMSE）
$RSE=\sqrt{\frac{\sum_{i=1}^n(y_i-\hat{y_i})^2}{(n)}}$
标准残差（RSE）
$RSE=\sqrt{\frac{\sum_{i=1}^n(y_i-\hat{y_i})^2}{(n-p-1)}}$
对于线性回归而言，均方根误差和标准残差之间的差异在实践中会非常小，尤其是在大数据应用中。
R 方统计量、t 统计量
$t_b=\frac{\hat{b}}{SE(\hat{b})}$

交叉验证——经典的统计回归度量（R方、F 统计量和 p值）都是“样本内”（in-sample）度量。k 折（fold）交叉验证的算法如下。
(1) 取出 1/k 的数据，作为验证样本。
(2) 用余下的数据训练模型。
(3) 将训练模型应用于验证集上（进行打分），并记录所需的模型评估指标。
(4) 将最初取出的 1/k 数据放回，再取出 1/k 数据，其中不包括上一次取出的任何记录。
(5) 重复第 2 步和第 3 步。
(6) 重复上述步骤，直至验证集使用了每个记录。
(7) 对模型评估度量取平均或进行组合。
上面将数据划分为训练样本和验证样本的过程，也被称为折。


### 模型选择和逐步回归法
奥卡姆剃刀原则（principle of Occam's razor）：在其他条件相同的情况下，应优先选用更简单的模型而不是更复杂的模型
添加额外的变量，几乎总会降低均方根误差并增大 R方

回归的 AIC 的计算：$AIC= 2P+nlog(RSS/n)$
P 是变量的数量，n 是记录的数量。目标是找出使 AIC 最小的模型。如果模型具有 k个额外变量，那么惩罚项为 2k。
AIC变体。
- AICc：针对小规模样本修正的 AIC。
- BIC（贝叶斯信息准则）：类似于 AIC，但是在模型中额外添加了变量，因此具有更强的惩罚。
- Mallows Cp：AIC 的一种变体，由 Colin Mallows 提出。



##### 模型选择

- 全子集回归法（all subset regression）【代价太高】
- stepAIC(使用了逐步回归法，通过连续地添加并丢弃预
测因子，发现可降低 AIC 的模型)【r的mass包】

惩罚回归的思想类似于 AIC。拟合模型的函数并不是显式地搜索一组离散的模型，而是添加了一个新限制，对有多个变量（参数）的模型进行惩罚。惩罚回归不像逐步回归、前向和后向选择那样要完全清除预测变量，而是通过减少系数来应用惩罚，在一些情况下，甚至会减少至接近于 0。常见的惩罚回归是岭回归和 LASSO 回归

全子集回归和逐步回归是“样本内”方法。这意味着模型选取可能会受限于过拟合，不能很好地应用于新数据。为了避免出现这一问题，一种常用的方法是使用交叉验证去验证模型。在线性回归中，过拟合通常不是大问题，因为线性回归对数据给出的是一种简单（线性）全局结构。对于更为复杂的模型而言，尤其是响应本地数据结构的迭代过程，交叉验证是一种非常重要的工具

#### 加权回归

- 适用情况
  - 反方差权重（当不同观测值使用了不同的精度测量时）
  - 分析聚合的数据，加权变量编码了聚合数据中每行代表了多少个原始观测值
加权回归用于拟合函数中，可以对特定记录给予更大或更小的权重。

###  回归中的因子变量
因子变量（factor variable）也称为分类变量，它是一组数量有限的离散
虚拟变量
　　二元的 0/1 变量，通过对因子数据重新编码得到，可用于回归模型或其他模型。
参考编码
　　统计学家最常使用的编码类型。它以因子的一层作为参考层，并将其他因子与参考
层进行对比。
　　同义词：编码处理
独热编码（one hot encoder）
　　机器学习领域中常用的一种编码。它保留了所有的因子层。虽然该编码适用于部分
机器学习算法，但并不适用于多元线性回归。
偏差编码
　　在编码中用于对比的并不是参考层，而是将每一层与整体均值进行对比。
　　同义词：总和对照（sum contrasts）编码


近邻算法和树模型中，独热编码是因子变量的标准表示方式


有序因子变量(因子变量体现出了因子的层级)

解释回归方程
相关变量
　　当预测变量高度相关时，难以解释单个回归系数。
多重共线性
　　当预测变量间存在完美的或近乎完美的相关性时，回归是不稳定的，或者说是不可能计算的。
　　同义词：共线性
混淆变量
　　一种重要的预测变量。忽视该变量可导致回归方程给出伪关系。
主效应
　　预测变量和结果变量之间的关系，该关系独立于其他的变量。
交互作用
　　两个或两个以上预测变量和响应之间的相互依赖关系。

对于树模型、聚类和最近邻等非回归方法，多重共线性可能并不会构成问题。在这些非回归方法中，可能会建议保留 P 个虚拟变量，而非 P-1 个。这就是说，即便是在这些方法中，非冗余的预测变量可能依然是个优点。


### 检验假设：回归诊断
鉴于离群值可能会在小规模数据集中导致问题，关注离群值主要是为了发现数据中存在的问题，或是确定异常所在。
- 单个记录（包括回归离群值）可以对小规模数据集的回归方程产生很大的影响。但是在大数据中，这种效果却荡然无存。
- 如果将回归模型用于形式推断（如 p 值等），那么应该检验对残差分布的一些假设。但是对于数据科学而言，残差分布通常无关紧要。
- 偏残差图可以用于定性地评估每个回归项的拟合情况，这可能会得出另一种模型声明


### 多项式回归和样条回归
多项式回归
　　在回归方程中添加了多项式项，例如平方项、三次方项等。
　　多项式回归只捕获了非线性关系的部分曲率。添加高阶项（例如三次方项），通常会导致回归线中出现不期望的“摇摆”（wiggliness）现象。

样条回归
　　使用一系列多项式片段去拟合一条平滑曲线。
结点
　　分隔样条片段的值。
广义加性模型
　　可以自动选择结点的样条模型。
　　同义词：GAM

 从本质上讲，所有响应不能表示为预测变量（或预测变量的某
种转换）的线性组合的模型，都是非线性的。非线性回归模型需要做数值优化，因此更难以拟合，计算的强度也更大。如有可能应尽量使用线性模型