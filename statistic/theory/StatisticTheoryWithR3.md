---
title: 实用统计学（三）
categories: [Statistic,Theory]
tags:
  - Statistic
  - R
toc: true
abbrlink: af33eebd
date: 2022-07-09 17:30:20
latex: true
---
<!--more-->
## 　统计机器学习
统计机器学习是数据驱动的，并不试图在数据上强加线性结构或其他的整体结构


### 　K最近邻算法
对于每个要进行分类或预测的记录，该算法：
(1) 找出 K 个具有相似特征（即具有相似的预测值）的记录。
(2) 对于分类，找出这些相似记录中的多数类，将其指定为新记录的类。
(3) 对于预测（KNN回归），找出这些相似记录的均值，并将该均值作为新记录的预测值。
K 最近邻算法的预测结果取决于特征的规模、相似性的测定方法以及 K 值的设置等因素。此外在K最近邻算法中，所有的预测变量必须是数值型的。

近邻——具有相似预测值的两个记录。
距离度量——以单一数值的形式，测量两个记录之间的距离。
标准化（归一化）——减去均值，并除以标准偏差。
z分数——标准化后得到的值。
K——在最近邻计算中考虑的近邻个数。
　
1.　距离度量
距离度量用于判定相似性（接近度），它是一个测量两个记录之间距离的函数。最广为使用的向量距离度量是欧氏距离。
在测量两个向量间的欧氏距离时，依次取两个向量中对应元素的差值，并对各个差值平方，累加后再取平方根
$\sqrt{(x_1-u_1)^2+ \dots+(x_p-u_p)^2}$
如果数据是数值型的，那么另一种常用的距离度量是曼哈顿距离
$|x_1-u_1| + \dots + |x_p-u_p|$

- 欧氏距离表示的是两点之间的直线距离
- 曼哈顿距离是在某一时刻以同一方向遍历两点之间的距离
    - 如遍历矩形城市街区,如果定义相似性为点到点的行程时间，那么曼哈顿距离更适用。欧氏距离和曼哈顿距离并未考虑相关性，只是对这些特征的属性添加了更大的权重
    - 在测量两个向量之间的距离时，测量值取决于规模相对较大的变量（特征）。马氏距离的缺点是在计算中要使用协方差矩阵，这增加了计算的难度和复杂性

1. 独热编码

2. 标准化
归一化、标准化
z 分数只是一种重新调整变量尺度的方式。均值可以替换成更稳健的位置估计量（中位数）
数据库标准化的目的在于去除冗余的数据，并验证数据的依赖关系

K 值设置过高，会对数据做过平滑，进而丧失了KNN的一个主要优点，即捕获数据局部结构的能力。KNN 本身在性能上并不具有竞争力。在实际的模型拟合中，可以将 KNN 作为一个阶段性过程，用于向其他分类方法中添加“局部知识”。具体做法如下：
(1) 在数据上运行 KNN，为每个记录生成一个分类（或是分类的拟概率）；
(2) 将结果作为一个新特征添加到记录中，然后在生成的数据上运行另一种分类方法。这样使用了原始预测变量两次。

- 预测变量通常需要做标准化，以避免大尺度变量主导了距离度量。
- KNN 常常作为预测建模过程的第一个阶段。KNN 的预测值会作为一个预测变量添加回数据中，进而用于第二阶段（非 KNN）的建模

### 树模型

简单树模型可以表示为预测变量之间的关系，易于解释
概念|定义
---|---
递归分区|反复对数据进行划分和细分，目的是使每个最终细分内的结果尽可能同质。（recursive partition）
拆分值|一个预测变量值，它将一组记录分为两部分，使得一部分中的预测变量小于拆分值，而另一部分中的预测变量大于拆分值。（split value）
节点|在决策树中（或在一组相应的分支规则中），节点是拆分值的图形化表示（或规则表示）。
叶子|一组 if-then 规则的终点，或一个树分支的终点。在树中访问叶子的规则，构成了对树中一条记录的分类规则。
损失|在拆分过程的某一阶段中误分类的个数。损失越大，不纯度越高。
不纯度(异质性)|表示在数据的一个细分中发现多个类混杂的程度。细分中混杂的类越多，该细分的不纯度就越高。(反义词：同质性、纯度)
剪枝|为了降低过拟合，对一棵完全长成树逐步剪枝的过程。

> 拟合树模型主要使用软件包 rpart 和 tree

1.　递归分区算法
  决策树的构造算法被称为递归分区法
  
1. 测量同质性或不纯度
  树模型递归地创建分区（记录的集合），并给出 Y = 0 或 Y = 1的预测结果。从递归算法中可以看到，还需要一种测量分区同质性（也称为类纯度）的方法。或者也可以说，需要测量分区的不纯度。预测的正确率是分区内误分类记录的比例 p。p 的取值介于0（即完美分区）和 0.5（即纯随机猜测）之间。
  对于不纯度来说，正确率并非一种很好的度量。
  两种常用的不纯度度量分别是基尼不纯度和熵（或信息）
  - 基尼不纯度
  - 熵 $I(A)= -plog_2(p)-(1-p)log_2(1-p)$
  
 基尼系数仅限于二分类问题，并与 AUC 度量有关
  
  
1. 剪枝  
    剪枝到验证数据集上的误差达到最小
    停止拆分
    - 如果拆分后的子分区过小，或末端叶子的规模过小，就应避免拆分。在 rpart 中，这些约束是由参数 minsplit 和 minbucket 控制的，默认值分别是 20和 7
    - 如果新分区并未使不纯度“显著”降低，那么就不必拆分该分区。在 rpart 中，这由复杂度参数 cp 控制。该参数度量了树模型的复杂度。树模型越复杂，cp 的值就越大。在实践中，参数cp是通过对树模型的额外复杂度（拆分）附加惩罚项，从而限制了树模型的增长

最常用的 cp 值估计方法是使用交叉验证
(1) 将数据分为训练集和验证集。
(2) 使用训练集生长树模型。
(3) 逐步剪枝，并在每步记录 cp 值（使用训练集）。
(4) 注意在验证集上取得最小误差（损失）的 cp 值。
(5) 将数据重新拆分为训练集和验证集，并重复树模型的生长、剪枝和记录 cp 过程。
(6) 重复执行上述步骤，对反映每个树的最小误差的 cp 值求平均值。
(7) 回到原始数据，也可以是将要处理的数据上，生长树模型，并在最优 cp 值处终止执行算法。
1.　预测连续值
使用树模型预测连续值（也被称为回归）时，遵循着同样的逻辑和过程，不同之处是使用每个子分区中距离均值的平方偏差（平方误差）来度量不纯度，并通过每个分区中的均方误差的平方根判断预测性能

- 完全长成树会过产生拟合，因此为了使模型捕获信号而非噪声，必须做剪枝。
- 虽然随机森林和 Boosting 等多树模型算法具有更好的预测性能，但失去了单个树模型基于规则的交流能力。

###　Bagging和随机森林
集成（模型平均）
　　使用一组模型给出预测。
　　主要变体——bagging 、Boosting
Bagging（自助法聚合）
　　对数据使用自助法构建一组模型的通用方法。
随机森林（自助法聚合决策树）
　　使用决策树的一类自助法聚合估计。
变量重要性
　　对预测变量在模型性能中重要性的测量
　　
#### Bagging方法

集成方法的基本实现
(1) 给定一个数据集，采用一种预测模型，并记录该模型的预测情况。
(2) 在同一数据集上，依次使用多个模型重复步骤 1。
(3) 对于每个要预测的记录，对预测值取均值（或加权均值，也可以使用多数票）

Bagging 方法和集成的基本算法类似，只是 Bagging 并不是要在同一数据上拟合所有的模型，而是对使用每个自助法重抽样拟合一个新模型。

#### 随机森林

随机森林是将 Bagging 方法应用于决策树，并做了一个重要的扩展。该算法不仅对记录做抽样，而且也对变量做抽样。
传统的决策树在确定如何将一个分区A拆分为子分区时，通过最小化基尼不纯度等标准去选择变量和拆分点。在随机森林算法中，每一阶段的变量选择受限于变量的一个随机子集。与基本的树算法相比，随机森林算法额外添加了两步，分别是Bagging方法，以及每次拆分时对变量的自助法抽样

(1) 从记录中做一次自助法（带放回的）抽样，得到一个子样本。
(2) 对于第一次拆分，无放回地随机抽样 p(p < P)个变量。
(3) 对于每组抽样变量 ，应用如下的拆分算法。
　　a.　对于$X_{j(k)}$ 的每个值$s_{j(k)}$ ： 将分区 A 中满足$X_{j(k)}\leq s_{j(k)}$ 的记录拆分为一个分区，其余满足$X_{j(k)}>= s_{j(k)}$的记录作为另一个分区；测量 A 的每个子分区中类的同质性。
　　b.　选择生成分区内最大类同质性的$X_{j(k)}$。
(4) 选择生成分区内最大类同质性的变量$X_{j(k)}$和拆分值$X_{j(k)}$ 。
(5) 继续下一次拆分，重复从步骤 2 开始的上述步骤。
(6) 遵循同一过程，继续拆分，直到得到一棵完全长成树。
(7) 返回步骤 1，再做一次自助法抽样，得到子样本，并重复上述过程

每一步需要抽样的变量数根据经验规则是选取$\sqrt{p}$个，其中 P 是预测变量的个数。

在为具有多个特征和记录的数据构建预测模型时，随机森林算法的强大得以尽显。
该算法可以自动确定重要的预测变量，并可以发现交互项所对应的预测变量之间的复杂关系

误差的包外（OOB）估计是指将训练得到的模型作用于训练集中未使用的数据上时，所得到的错误率。使用模型的输出，可以绘制出 OOB 误差与随机森林中树模型的数量

1. 超参数
参数是一些需要在拟合模型前设置的参数，它们并不会在算法训练过程中得到优化。
随机森林具有一组超参数。可以使用交叉验证调整超参数，以避免产生过拟合。

nodesize
　　末端节点（即树的叶子）的最小规模。对于分类，默认值为 1；对于回归，默认值为 5。
maxnodes
　　每个决策树中的最大节点数。默认情况下，没有限制。
需拟合的最大树会受到nodesize 的限制。当增大 nodesize 或设置maxnodes后，算法将拟合一个较小的树模型，而且不太可能给出假的预测规则。可以使用交叉验证），检验设置不同超参数值的效果。
　　
#### Boosting
1.　Boosting算法
概念|定义
---|---
Boosting|在拟合一组模型时所使用的一种通用方法。Boosting 在每轮连续的拟合中，会对具有更大残差的记录赋予更大的权重。
Adaboost|Boosting 算法的一种早期实现，它根据残差的情况对数据重新加权。
梯度提升|一种更通用的 Boosting 算法。它将问题转化为代价函数最小化的问题。
随机梯度提升（SGD）|最常用的 Boosting 算法。它在每轮拟合中加入了对记录和数据列的重抽样
1.　XGBoost
软件包中的函数 xgboost 提供了多个可调整并且应该调整的两个非常重要的参数是 subsample 和 eta。
subsample 参数控制每次迭代时应该被抽样的部分观测值
eta 设置了 Boosting 算法中 的收缩因子

- xgboost 不支持公式语法，因此需要将预测变量转换为 R 语言的 data.matrix 对象
- 响应变量需要转换为 0/1 二元变量
- 参数 objective 指定了 xgboost 函数所处理的问题类型
- xgboost 根据该参数选取优化指标

1. 正则化：避免过拟合
对模型的复杂性添加惩罚项，有助于避免产生过拟合。最小二乘回归会最小化残差平方和（RSS）。而岭回归则最小化残差平方和，并对系数的数量和大小添加惩罚项

1. 超参数和交叉验证
交叉验证指出，使用了较小的eta值并且层数较少的树模型，会得出更准确的结果。由于这样的模型也更稳定，所以应使用的最佳参数是 eta = 0.1，max_depth = 3（也可能是max_depth = 6）

arg|meaning
---|---
eta|值位于 0 和 1 之间的收缩因子，即 Boosting 算法的 α。默认值为 0.3。但是对于噪声数据，推荐使用更小的值，例如 0.1。
nrounds|设置 Boosting 算法的循环次数。如果将eta设置为一个较小的值，这会增加循环的次数，因为算法学习的速度更慢。只要在计算中设置了一些防止过拟合的参数，那么运行更多轮循环也没问题。
max_depth|设置树模型的最大深度，默认值为 6。与随机森林拟合非常深的树模型不同，Boosting 算法通常会拟合一个层数不多的树模型。这样做的优点是，可以避免由噪声数据导致模型中出现虚假的复杂交互。
subsample 和 colsample_bytree
　　subsample 指定了做无放回抽样的部分记录，colsample_bytree指定了在拟合树模型中要抽样的部分预测变量。这些参数类似于随机森林中使用的相应参数，有助于避免产生过拟合。
lambda 和 alpha
　　帮助控制过拟合的正则化参数,分别表示曼哈顿距离和欧氏距离的平方,增大这两个参数将会惩罚更复杂的模型，并减小拟合树模型的规模。
###　无监督学习
####　主成分分析
主成分
　　预测变量的一种线性组合。
载荷（权重）
　　将预测因子转换为成分的过程中所使用的权重值
陡坡图（碎石图）
　　一种展示各成分方差的绘图，图中显示了各成分的相对重要性主成分分析的基本理念是，将多个数值型预测变量组合成一组规模较小的变量，它们是原始变量的加权线性组合。所形成的规模较小的一组变量被称为主成分。
主成分可以“解释”完整变量集的大部分变异性，同时降低数据维度。在构建主成分中所使用的权重，体现了原始变量对新的主成分的相对贡献
在 R 语言中，可以使用函数 princomp 计算主成分
1.　计算主成分
1.　解释主成分

####　K-Means聚类

 概念 | 定义
 --|--
类（cluster）|一组类似的记录。
类均值|表示类内记录变量均值的向量。
K|类的个数
一个类内每个记录到该类均值之间距离的平方和为类内平方和
K-Means 通过最小化类内平方和，将数据划分为 K 个类。
K-Means 并不能保证各个类的规模相同，但是能找出相互分离情况最好的类。
应对连续变量做归一化（标准化），一般做法是减去均值再除以标准偏差
类均值（cluster mean）并非指单个数字，而是指表示变量均值的向量。


####　层次聚类（系统聚类）
层次聚类比 K-Means 更灵活，并且更易于应用在非数值型变量上，对于发现离群的或异常的组和记录也更为敏感
层次聚类也适于做直观的图形展示，因而解释类也更为容易
但不能很好地扩展到具有数百万条记录的大规模数据集，层次聚类的大部分应用都集中在一些规模相对较小的数据集上

- 层次聚类算法开始时，每条记录单独构成一个类。
- 在凝聚算法中，类逐步与相邻的类合并，直到所有记录属于单一类。
-凝聚算法的类历史可以被保留并绘制出来。用户（无须预先指定类数）可以在算法执行的各个阶段，可视化地查看类数和类的结构。
- 有多种方法可以计算类之间的距离。这些方法都依赖于所有记录间距离


####　基于模型的聚类
最广为使用的基于模型的聚类方法依赖于多元正态分布
基于模型的聚类的关键思想是，假定每条记录的分布符合 K 个多元正态分布之一，其中 K 是类的个数。每个分布具有不同的均值$\mu$和协方差矩阵$\sum$
R 语言的 mclust 软件包提供了丰富的基于模型聚类的功能。

不同于 K-Means 和层次聚类，mclust 会自动选取类数

通过选取使贝叶斯信息准则（BIC）值最大的类数实现的。BIC（类似于 AIC）是在一组候选模型中找到最佳模型的通用工具。
例如，AIC（或 BIC）常用于在逐步回归中选择模型。BIC通过对模型中的参数数量添加一个惩罚项，选择最优拟合模型。在基于模型的聚类中，增加类数总是会提高模型的拟合度，但代价是在模型中额外地引入了一些参数


####　变量的缩放和分类变量

##### 高氏距离（Gower's distance）
　　一种应用于数值数据和类别数据相混合的缩放算法。它可以将所有变量缩放到 [0, 1]范围内
高氏距离的基本思想是，根据数据类型，对每个变量应用不同的距离度量。
- 对于数值型变量和有序因子，高氏距离计算为两条记录间差异的绝对值（即曼哈顿距离）。
- 对于分类变量，如果两个记录属于不同的类，那么距离为 1；如果它们属于同一个分类，那么距离为 0。

流程
1. 对每条记录，计算所有变量对 i 和 j 间的距离$d_{ij}$ 
1. 将每个距离 缩放到区间 [0, 1] 中。
1. 使用简单均值或加权均值，将所有变量对间的缩放距离相加，创建一个距离矩阵。

##### 混合数据的聚类问题
K-Means 和主成分分析最适合用于连续变量。
对于较小的数据集，使用基于高氏距离的层次聚类更好。
从原理上看，K-Means 完全适用于二元数据和分类数据。通常会使用“独热编码”，将分类数据转换为数值型数据。==然而在实践中，很难对二元数据应用K-Means 和主成分分析。==
如果使用标准的 z 分数，那么二元变量将会主导聚类的定义。这是因为 0/1 变量只有两个值，
而 K-Means 是通过将所有的 0 或 1 记录指定给聚类，获得较小的类内平方和